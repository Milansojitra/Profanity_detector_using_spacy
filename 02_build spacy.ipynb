{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  In this notebook we set rules in spacy model to detect profanity words and based on that predict the degree of profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "import spacy_transformers\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>severity_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbie</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abeed</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aboe</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaner</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beaners</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>wetbacks</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>wigger</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>wop</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>wophead</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>zipperhead</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           text severity_description\n",
       "0         abbie                 Mild\n",
       "1         abeed               Strong\n",
       "2          aboe                 Mild\n",
       "3        beaner               Severe\n",
       "4       beaners               Severe\n",
       "..          ...                  ...\n",
       "170    wetbacks               Severe\n",
       "171      wigger               Severe\n",
       "172         wop               Strong\n",
       "173     wophead               Strong\n",
       "174  zipperhead               Strong\n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the racial_word file\n",
    "df = pd.read_csv(\"racial_word.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all 3 level of profanity words\n",
    "label = df.groupby('severity_description')\n",
    "mild_list = label.get_group(\"Mild\")['text'].to_list()\n",
    "strong_list = label.get_group(\"Strong\")['text'].to_list()\n",
    "severe_list = label.get_group(\"Severe\")['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patterns \n",
    "mild_patterns = [nlp(text) for text in mild_list]\n",
    "strong_patterns = [nlp(text) for text in strong_list]\n",
    "severe_patterns = [nlp(text) for text in severe_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'profanity' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'profanity']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-e5c68cac6c24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Add the component to the pipeline after the \"ner\" component\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"profanity_component\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"profanity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# We're loading the component from a model. After loading the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'profanity' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'profanity']"
     ]
    }
   ],
   "source": [
    "# Define the custom component to detect profanity in each sentence\n",
    "@Language.component(\"profanity_component\")\n",
    "def profanity_component_function(doc):\n",
    "    \n",
    "    #inner function to get the degreee of profanity.\n",
    "    def get_label(obj):\n",
    "        matcher.add(\"Mild\",mild_patterns)\n",
    "        matcher.add(\"Strong\",strong_patterns)\n",
    "        matcher.add(\"Severe\",severe_patterns)\n",
    "\n",
    "        if any([t.text in severe_list for t in obj]):\n",
    "            return \"Severe\"\n",
    "        elif any([t.text in strong_list for t in obj]):\n",
    "            return \"Strong\"\n",
    "        elif any([t.text.lower() in mild_list for t in obj]):\n",
    "            return \"Mild\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "    \n",
    "    # check in each of the sentence and assigne the custom component\n",
    "    for sent in doc.sents:\n",
    "        sent.set_extension(\"Profanity_level\", getter=get_label, force = True)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"profanity_component\", name=\"profanity\", after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x191a46cd880>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x191a46cdca0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x191a3ad8dd0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x191a59c8a80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x191a59c8940>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x191a3c50f20>),\n",
       " ('profanity', <function __main__.profanity_component_function(doc)>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you Aboe.', 'Mild'), ('you are zip in the wire.', 'Neutral')]\n"
     ]
    }
   ],
   "source": [
    "# Add the component to the pipeline after the \"ner\" component\n",
    "doc = nlp(\"you Aboe. you are zip in the wire.\")\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "print([(sent.text, sent._.Profanity_level) for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store that model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"profaniity_model\" \n",
    "nlp.to_disk(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating en_core_web_sm.egg-info\n",
      "writing en_core_web_sm.egg-info\\PKG-INFO\n",
      "writing dependency_links to en_core_web_sm.egg-info\\dependency_links.txt\n",
      "writing entry points to en_core_web_sm.egg-info\\entry_points.txt\n",
      "writing requirements to en_core_web_sm.egg-info\\requires.txt\n",
      "writing top-level names to en_core_web_sm.egg-info\\top_level.txt\n",
      "writing manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "reading manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'en_core_web_sm.egg-info\\SOURCES.txt'\n",
      "running check\n",
      "creating en_core_web_sm-3.2.0\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\attribute_ruler\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\lemmatizer\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\lemmatizer\\lookups\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\ner\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\parser\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\senter\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tagger\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tok2vec\n",
      "creating en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "copying files to en_core_web_sm-3.2.0...\n",
      "copying MANIFEST.in -> en_core_web_sm-3.2.0\n",
      "copying README.md -> en_core_web_sm-3.2.0\n",
      "copying meta.json -> en_core_web_sm-3.2.0\n",
      "copying setup.py -> en_core_web_sm-3.2.0\n",
      "copying en_core_web_sm\\__init__.py -> en_core_web_sm-3.2.0\\en_core_web_sm\n",
      "copying en_core_web_sm\\meta.json -> en_core_web_sm-3.2.0\\en_core_web_sm\n",
      "copying en_core_web_sm\\prof_component.py -> en_core_web_sm-3.2.0\\en_core_web_sm\n",
      "copying en_core_web_sm.egg-info\\PKG-INFO -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\SOURCES.txt -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\dependency_links.txt -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\entry_points.txt -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\not-zip-safe -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\requires.txt -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm.egg-info\\top_level.txt -> en_core_web_sm-3.2.0\\en_core_web_sm.egg-info\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\README.md -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\config.cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\meta.json -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\tokenizer -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\attribute_ruler\\patterns -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\attribute_ruler\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\lemmatizer\\lookups\\lookups.bin -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\lemmatizer\\lookups\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\ner\\cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\ner\\model -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\ner\\moves -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\ner\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\parser\\cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\parser\\model -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\parser\\moves -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\parser\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\senter\\cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\senter\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\senter\\model -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\senter\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\tagger\\cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tagger\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\tagger\\model -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tagger\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\tok2vec\\cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tok2vec\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\tok2vec\\model -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\tok2vec\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\\key2row -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\\lookups.bin -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\\strings.json -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\\vectors -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "copying en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\\vectors.cfg -> en_core_web_sm-3.2.0\\en_core_web_sm\\en_core_web_sm-3.2.0\\vocab\n",
      "Writing en_core_web_sm-3.2.0\\setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'en_core_web_sm-3.2.0' (and everything under it)\n",
      "[i] Building package artifacts: sdist\n",
      "[+] Including 1 Python module(s) with custom code\n",
      "[+] Loaded meta.json from file\n",
      "profaniity_model\\meta.json\n",
      "[+] Generated README.md from meta.json\n",
      "[+] Successfully created package directory 'en_core_web_sm-3.2.0'\n",
      "prof_model\\en_core_web_sm-3.2.0\n",
      "[+] Successfully created zipped Python package\n",
      "prof_model\\en_core_web_sm-3.2.0\\dist\\en_core_web_sm-3.2.0.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 17:44:26.722073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-16 17:44:26.722121: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "warning: no files found matching 'LICENSE'\n",
      "warning: no files found matching 'LICENSES_SOURCES'\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy package profaniity_model prof_model --code prof_component.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# you have to import this model step by step for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ............................................. End......................................................"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
